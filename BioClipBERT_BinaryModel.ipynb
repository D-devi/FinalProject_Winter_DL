{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29f692ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29f692ba",
        "outputId": "c3eaae0c-c0f8-455a-b2d5-cf2adc34fc29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "hello\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from datasets import Dataset, DatasetDict\n",
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility for PyTorch and NumPy.\n",
        "\n",
        "    Args:\n",
        "        seed_value (int): The seed value to set for random number generators.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "set_seed(42)\n",
        "print(\"hello\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ceed734",
      "metadata": {
        "id": "8ceed734"
      },
      "outputs": [],
      "source": [
        "\n",
        "path_project = '/content/drive/MyDrive/Project'\n",
        "\n",
        "# 2) Load + preprocess + split\n",
        "def vqa_rad_setup(path_project):\n",
        "    json_file = \"VQA_RAD Dataset Public.json\"\n",
        "    image_folder = \"VQA_RAD Image Folder\"\n",
        "    json_path = os.path.join(path_project, json_file)\n",
        "    image_dir  = os.path.join(path_project, image_folder)\n",
        "\n",
        "\n",
        "\n",
        "    df = pd.read_json(json_path)\n",
        "    # build full image paths\n",
        "    df['image_path'] = df['image_name'].apply(lambda fn: os.path.join(image_dir, fn))\n",
        "\n",
        "    total_rows     = len(df)\n",
        "    print(f\"Total rows in df_binary:      {total_rows}\")\n",
        "\n",
        "\n",
        "    num_organs = df['image_organ'].nunique()\n",
        "    print(f\"There are {num_organs} distinct organs.\")\n",
        "\n",
        "    # 3) if you want to see them all:\n",
        "    print(\"Organ list:\", df['image_organ'].unique())\n",
        "\n",
        "    # filter to yes/no and map to 0/1\n",
        "    df['answer'] = df['answer'].str.strip().str.lower()\n",
        "    df_binary = df[df['answer'].isin(['yes', 'no'])].copy()\n",
        "    df_binary['label'] = df_binary['answer'].map({'yes': 1, 'no': 0})\n",
        "\n",
        "    # shuffle, select columns, print\n",
        "    df_binary = (\n",
        "        df_binary[['image_path', 'question', 'label']]\n",
        "        .sample(frac=1, random_state=42)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    total_rows     = len(df_binary)\n",
        "    print(f\"Total rows in df_binary:      {total_rows}\")\n",
        "    return df_binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "455185a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "455185a5",
        "outputId": "f45a4c47-0010-4719-fb91-8d3a899ccb8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows in df_binary:      2248\n",
            "There are 3 distinct organs.\n",
            "Organ list: ['HEAD' 'CHEST' 'ABD']\n",
            "Total rows in df_binary:      1193\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          image_path  \\\n",
              "0  /content/drive/MyDrive/Project/VQA_RAD Image F...   \n",
              "1  /content/drive/MyDrive/Project/VQA_RAD Image F...   \n",
              "2  /content/drive/MyDrive/Project/VQA_RAD Image F...   \n",
              "3  /content/drive/MyDrive/Project/VQA_RAD Image F...   \n",
              "4  /content/drive/MyDrive/Project/VQA_RAD Image F...   \n",
              "5  /content/drive/MyDrive/Project/VQA_RAD Image F...   \n",
              "6  /content/drive/MyDrive/Project/VQA_RAD Image F...   \n",
              "7  /content/drive/MyDrive/Project/VQA_RAD Image F...   \n",
              "8  /content/drive/MyDrive/Project/VQA_RAD Image F...   \n",
              "9  /content/drive/MyDrive/Project/VQA_RAD Image F...   \n",
              "\n",
              "                                           question  label  \n",
              "0   Are the patients' ribs symmetric on both sides?      0  \n",
              "1  Are there cilia present at the level of alveoli?      0  \n",
              "2                            Is this coronal plane?      1  \n",
              "3                        Is the patient lying down?      1  \n",
              "4  Do you see a cavitary lesion in this chest xray?      1  \n",
              "5            Is there free air under the diaphragm?      0  \n",
              "6                      is there tracheal deviation?      0  \n",
              "7            Is this in the lumbar vertebral level?      1  \n",
              "8            Does this patient have a pneumothorax?      0  \n",
              "9               Was this patient given IV contrast?      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-945c75d0-cd7c-45da-974b-65b92fb7df22\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>question</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/Project/VQA_RAD Image F...</td>\n",
              "      <td>Are the patients' ribs symmetric on both sides?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/Project/VQA_RAD Image F...</td>\n",
              "      <td>Are there cilia present at the level of alveoli?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/Project/VQA_RAD Image F...</td>\n",
              "      <td>Is this coronal plane?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/Project/VQA_RAD Image F...</td>\n",
              "      <td>Is the patient lying down?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/Project/VQA_RAD Image F...</td>\n",
              "      <td>Do you see a cavitary lesion in this chest xray?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>/content/drive/MyDrive/Project/VQA_RAD Image F...</td>\n",
              "      <td>Is there free air under the diaphragm?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>/content/drive/MyDrive/Project/VQA_RAD Image F...</td>\n",
              "      <td>is there tracheal deviation?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>/content/drive/MyDrive/Project/VQA_RAD Image F...</td>\n",
              "      <td>Is this in the lumbar vertebral level?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>/content/drive/MyDrive/Project/VQA_RAD Image F...</td>\n",
              "      <td>Does this patient have a pneumothorax?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>/content/drive/MyDrive/Project/VQA_RAD Image F...</td>\n",
              "      <td>Was this patient given IV contrast?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-945c75d0-cd7c-45da-974b-65b92fb7df22')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-945c75d0-cd7c-45da-974b-65b92fb7df22 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-945c75d0-cd7c-45da-974b-65b92fb7df22');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b796972d-a863-4e1d-b914-a0ea581a60dd\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b796972d-a863-4e1d-b914-a0ea581a60dd')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b796972d-a863-4e1d-b914-a0ea581a60dd button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_binary",
              "summary": "{\n  \"name\": \"df_binary\",\n  \"rows\": 1193,\n  \"fields\": [\n    {\n      \"column\": \"image_path\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 294,\n        \"samples\": [\n          \"/content/drive/MyDrive/Project/VQA_RAD Image Folder/synpic34947.jpg\",\n          \"/content/drive/MyDrive/Project/VQA_RAD Image Folder/synpic31757.jpg\",\n          \"/content/drive/MyDrive/Project/VQA_RAD Image Folder/synpic53228.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1029,\n        \"samples\": [\n          \"Besides the mass in the temporal horn, are there other enhancements in the image?\",\n          \"Is this film taken in a PA modality?\",\n          \"Is this a MRI of the chest?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df_binary = vqa_rad_setup(path_project)\n",
        "\n",
        "df_binary.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc023aff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc023aff",
        "outputId": "82a8f3be-8a75-4e38-c82a-cf02ecdb3bc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Collecting open_clip_torch\n",
            "  Downloading open_clip_torch-2.32.0-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (0.21.0+cu124)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (2024.11.6)\n",
            "Collecting ftfy (from open_clip_torch)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (0.30.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (0.5.3)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (1.0.15)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.9.0->open_clip_torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.9.0->open_clip_torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.9.0->open_clip_torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.9.0->open_clip_torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.9.0->open_clip_torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.9.0->open_clip_torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.9.0->open_clip_torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.9.0->open_clip_torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.9.0->open_clip_torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.9.0->open_clip_torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9.0->open_clip_torch) (1.3.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->open_clip_torch) (0.2.13)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open_clip_torch) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open_clip_torch) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open_clip_torch) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->open_clip_torch) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->open_clip_torch) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9.0->open_clip_torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open_clip_torch) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open_clip_torch) (2025.1.31)\n",
            "Downloading open_clip_torch-2.32.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m124.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ftfy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, open_clip_torch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed ftfy-6.3.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 open_clip_torch-2.32.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install open_clip_torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd118527",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd118527",
        "outputId": "75e2cec2-0c6d-4803-c08d-30c05428c6a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images (448px): 100%|██████████| 1193/1193 [00:20<00:00, 58.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Processed 1193 images; skipped 0.\n",
            "📐 full_image_tensor.shape = torch.Size([1193, 3, 448, 448])\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import open_clip\n",
        "\n",
        "# 1) Get preprocess from open_clip\n",
        "import open_clip\n",
        "model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(\n",
        "    'hf-hub:luhuitong/CLIP-ViT-L-14-448px-MedICaT-ROCO'\n",
        ")\n",
        "\n",
        "# 2) Image transformation function\n",
        "def transform_image_pre(image_path):\n",
        "    image = Image.open(image_path)\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "    return preprocess_train(image)  # Already returns [3, 448, 448] tensor\n",
        "\n",
        "# 3) Batch-process images\n",
        "image_tensors = []\n",
        "failed_images = []\n",
        "\n",
        "for img_path in tqdm(df_binary['image_path'], desc=\"Processing images (448px)\"):\n",
        "    try:\n",
        "        tensor = transform_image_pre(img_path)\n",
        "        image_tensors.append(tensor)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Skipped {img_path}: {e}\")\n",
        "        failed_images.append(img_path)\n",
        "\n",
        "# 4) Stack tensors\n",
        "if image_tensors:\n",
        "    full_image_tensor = torch.stack(image_tensors, dim=0)\n",
        "    print(f\"\\n✅ Processed {len(image_tensors)} images; skipped {len(failed_images)}.\")\n",
        "    print(\"📐 full_image_tensor.shape =\", full_image_tensor.shape)\n",
        "else:\n",
        "    print(\"❌ No images were successfully processed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a9780d3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a9780d3",
        "outputId": "8d560111-8091-4865-93da-16a6229055a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded BERT tokenizer with vocab size: 30522\n",
            "\n",
            "Tokenized 1193 questions.\n",
            "\n",
            "✅ Shape of Text_tensor: torch.Size([1193, 64])\n",
            "✅ Shape of Attention Mask Tensor: torch.Size([1193, 64])\n",
            "ℹ️  Max token ID: 29561 < Vocab Size: 30522? Yes ✅\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "vocab_size = tokenizer.vocab_size\n",
        "print(f\"Loaded BERT tokenizer with vocab size: {vocab_size}\")\n",
        "\n",
        "\n",
        "# Set max sequence length for tokenization\n",
        "max_seq_length = 64  # or any other value appropriate for your dataset\n",
        "\n",
        "# Tokenize all questions\n",
        "all_tokenized_texts = []\n",
        "for description in df_binary['question']:\n",
        "    tokenized_text = tokenizer(\n",
        "        description,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_seq_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    all_tokenized_texts.append(tokenized_text)\n",
        "\n",
        "print(f\"\\nTokenized {len(all_tokenized_texts)} questions.\")\n",
        "\n",
        "# Stack all input_ids and attention_mask tensors\n",
        "Text_tensor = torch.cat([item['input_ids'] for item in all_tokenized_texts], dim=0)  # [N, max_seq_length]\n",
        "Text_attention_mask_tensor = torch.cat([item['attention_mask'] for item in all_tokenized_texts], dim=0)\n",
        "\n",
        "# Final checks\n",
        "print(f\"\\n✅ Shape of Text_tensor: {Text_tensor.shape}\")                             # [N, max_seq_length]\n",
        "print(f\"✅ Shape of Attention Mask Tensor: {Text_attention_mask_tensor.shape}\")       # [N, max_seq_length]\n",
        "print(f\"ℹ️  Max token ID: {Text_tensor.max().item()} < Vocab Size: {vocab_size}? {'Yes ✅' if Text_tensor.max().item() < vocab_size else 'No ❌'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "818f6fd8",
      "metadata": {
        "id": "818f6fd8"
      },
      "outputs": [],
      "source": [
        "y_train_tensor = torch.tensor(df_binary['label'], dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04c919c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04c919c4",
        "outputId": "bd3da9b1-e1f8-4490-e189-a9c5abc57b05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original PIL image size (w, h): (1024, 1310)\n",
            "After transform, tensor shape: torch.Size([3, 448, 448])\n"
          ]
        }
      ],
      "source": [
        "first_path = df_binary['image_path'].iloc[0]\n",
        "\n",
        "# 1) Open & print original size\n",
        "img = Image.open(first_path)\n",
        "print(\"Original PIL image size (w, h):\", img.size)\n",
        "\n",
        "# 2) Apply your preprocessing & print tensor shape\n",
        "tensor = transform_image_pre(first_path)  # or preprocess_train(img)\n",
        "print(\"After transform, tensor shape:\", tensor.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21ee6a08",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21ee6a08",
        "outputId": "fcab60f2-cf65-4ca9-f611-b39997b5084f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSdpaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertModel\n",
        "import open_clip\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(\n",
        "    'hf-hub:luhuitong/CLIP-ViT-L-14-448px-MedICaT-ROCO'\n",
        ")\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "bert_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26282cce",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26282cce",
        "outputId": "2fcb6326-c4c4-4b8b-98a1-3f751b567814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1193/1193 [01:09<00:00, 17.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ image_features shape: torch.Size([1193, 768])\n",
            "✅ text_features shape: torch.Size([1193, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Feature extraction function\n",
        "def data_all_features(image_data, text_data, text_attention):\n",
        "    image_features = []\n",
        "    text_features = []\n",
        "\n",
        "    for img, input_ids, attention_mask in tqdm(zip(image_data, text_data, text_attention), total=len(image_data)):\n",
        "        img = img.unsqueeze(0).to(device)\n",
        "        input_ids = input_ids.unsqueeze(0).to(device)\n",
        "        attention_mask = attention_mask.unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Get image features from CLIP\n",
        "            #img_feat = model.get_image_features(pixel_values=img)  # [1, 1024]\n",
        "            img_feat = model.encode_image(img)\n",
        "            # Get text features from BERT\n",
        "            txt_outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            txt_feat = txt_outputs.pooler_output  # [1, 768]\n",
        "\n",
        "        image_features.append(img_feat.squeeze(0))  # [1024]\n",
        "        text_features.append(txt_feat.squeeze(0))   # [768]\n",
        "\n",
        "    return torch.stack(image_features), torch.stack(text_features)\n",
        "\n",
        "# Run feature extraction\n",
        "image_features, text_features = data_all_features(\n",
        "    full_image_tensor, Text_tensor, Text_attention_mask_tensor\n",
        ")\n",
        "\n",
        "# Shape check\n",
        "print(f\"✅ image_features shape: {image_features.shape}\")  # [N, 1024]\n",
        "print(f\"✅ text_features shape: {text_features.shape}\")    # [N, 768]\n",
        "\n",
        "# Split\n",
        "N = 75\n",
        "Image_train = image_features[N:]\n",
        "Image_test = image_features[:N]\n",
        "Text_train = text_features[N:]\n",
        "Text_test = text_features[:N]\n",
        "y_train = y_train_tensor[N:].to(device)\n",
        "y_test = y_train_tensor[:N].to(device)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e640b565",
      "metadata": {
        "id": "e640b565"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 128\n",
        "train_dataset = TensorDataset(Image_train, Text_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(Image_test, Text_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea7368cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea7368cd",
        "outputId": "21047e2a-d37e-4654-a42f-77320b08a86b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1118, 768])\n",
            "torch.Size([1118, 768])\n",
            "torch.Size([1118])\n"
          ]
        }
      ],
      "source": [
        "print(train_loader.dataset.tensors[0].shape)\n",
        "print(train_loader.dataset.tensors[1].shape)\n",
        "print(train_loader.dataset.tensors[2].shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CoAttentionFusionClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_dim: int,\n",
        "        text_dim: int,\n",
        "        fusion_dim: int = 512,\n",
        "        num_heads: int = 8,\n",
        "        dropout: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # project each modality into the same hidden space\n",
        "        self.img_proj = nn.Linear(image_dim, fusion_dim)\n",
        "        self.txt_proj = nn.Linear(text_dim, fusion_dim)\n",
        "\n",
        "        # cross‑attention layers\n",
        "        # image queries, text keys/values\n",
        "        self.attn_img2txt = nn.MultiheadAttention(\n",
        "            embed_dim=fusion_dim, num_heads=num_heads, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        # text queries, image keys/values\n",
        "        self.attn_txt2img = nn.MultiheadAttention(\n",
        "            embed_dim=fusion_dim, num_heads=num_heads, dropout=dropout, batch_first=True\n",
        "        )\n",
        "\n",
        "        # classification head\n",
        "        self.dropout    = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(fusion_dim, 1)\n",
        "\n",
        "    def forward(self, img_feat: torch.Tensor, txt_feat: torch.Tensor):\n",
        "        \"\"\"\n",
        "        img_feat: (B, image_dim)\n",
        "        txt_feat: (B, text_dim)\n",
        "        \"\"\"\n",
        "        # 1) project and add sequence dim → (B, 1, fusion_dim)\n",
        "        img = self.img_proj(img_feat).unsqueeze(1)\n",
        "        txt = self.txt_proj(txt_feat).unsqueeze(1)\n",
        "\n",
        "        # 2) co‑attention\n",
        "        # img attends to text:\n",
        "        img2txt, _ = self.attn_img2txt(query=img, key=txt, value=txt)\n",
        "        # text attends to image:\n",
        "        txt2img, _ = self.attn_txt2img(query=txt, key=img, value=img)\n",
        "\n",
        "        # 3) fuse by averaging both attended outputs and pooling the sequence dim\n",
        "        # each is (B,1,fusion_dim) → stack → (B,2,fusion_dim)\n",
        "        fused_seq = torch.cat([img2txt, txt2img], dim=1)\n",
        "        fused     = fused_seq.mean(dim=1)    # (B, fusion_dim)\n",
        "\n",
        "        # 4) classification\n",
        "        x      = self.dropout(fused)\n",
        "        logit  = self.classifier(x).squeeze(1)  # (B,)\n",
        "        return logit\n"
      ],
      "metadata": {
        "id": "m9RXlQzHj1hU"
      },
      "id": "m9RXlQzHj1hU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "image_dim = image_features.shape[1]\n",
        "text_dim  = text_features.shape[1]"
      ],
      "metadata": {
        "id": "b_VId8h_j3kl"
      },
      "id": "b_VId8h_j3kl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from itertools import product\n",
        "lrs = [1e-3, 1e-4, 1e-5]\n",
        "dropouts = [0.1, 0.2, 0.3]\n",
        "n_epochs = 50 # fewer epochs for quick tuning\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2) helper to train & eval one config\n",
        "def run_trial(lr, dropout, img2txt_mult, txt2img_mult):\n",
        "    # re‑init model\n",
        "    model = CoAttentionFusionClassifier(\n",
        "        image_dim=image_dim,\n",
        "        text_dim=text_dim,\n",
        "        fusion_dim=512,\n",
        "        num_heads=64,\n",
        "        dropout=dropout\n",
        "    ).to(device)\n",
        "\n",
        "    # 2) Define Parameter Groups (this is where you change the LR multipliers):\n",
        "    param_groups = [\n",
        "        {'params': [p for n, p in model.named_parameters() if 'attn_img2txt' in n], 'lr': lr * img2txt_mult}, # Fixed multiplier\n",
        "        {'params': [p for n, p in model.named_parameters() if 'attn_txt2img' in n], 'lr': lr * txt2img_mult}, # Fixed multiplier\n",
        "        {'params': [p for n, p in model.named_parameters() if 'attn' not in n]}, # Default LR for other params\n",
        "    ]\n",
        "\n",
        "    # 3) Initialize Optimizer with Parameter Groups:\n",
        "    opt = optim.Adam(param_groups, lr=lr)\n",
        "    crit = nn.BCEWithLogitsLoss()\n",
        "    best_f1 = 0.0\n",
        "    best_acc = 0.0\n",
        "    best_auroc = 0.0\n",
        "\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        # train one epoch\n",
        "        model.train()\n",
        "        for imgs, txts, labs in train_loader:\n",
        "            imgs, txts = imgs.to(device), txts.to(device)\n",
        "            labs = labs.to(device).float()\n",
        "            opt.zero_grad()\n",
        "            logits = model(imgs, txts)\n",
        "            loss = crit(logits, labs)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        # eval\n",
        "        model.eval()\n",
        "        all_p, all_l = [], []\n",
        "        with torch.no_grad():\n",
        "            for imgs, txts, labs in test_loader:\n",
        "                imgs, txts = imgs.to(device), txts.to(device)\n",
        "                logits = model(imgs, txts)\n",
        "                preds = (torch.sigmoid(logits) > 0.5).long()\n",
        "                all_p.extend(preds.cpu().tolist())\n",
        "                all_l.extend(labs.tolist())\n",
        "\n",
        "        f1 = f1_score(all_l, all_p)\n",
        "        acc = accuracy_score(all_l, all_p)\n",
        "        auroc = roc_auc_score(all_l, all_p)\n",
        "        best_f1 = max(best_f1, f1)\n",
        "        best_acc = max(best_acc, acc)\n",
        "        best_auroc = max(best_auroc, auroc)\n",
        "\n",
        "    return best_f1, best_acc, best_auroc\n",
        "\n",
        "# 3) grid‐search\n",
        "# You also need to define img2txt_mults and txt2img_mults\n",
        "img2txt_mults = [0.5, 1.0, 2.0, 3.0]\n",
        "txt2img_mults = [0.5, 1.0, 2.0, 5.0]\n",
        "\n",
        "results = []\n",
        "for lr, dp, img2txt_mult, txt2img_mult in product(lrs, dropouts, img2txt_mults, txt2img_mults):\n",
        "    f1, acc, auroc = run_trial(lr, dp, img2txt_mult, txt2img_mult)  # Get all metrics\n",
        "    print(f\" → lr={lr:.0e}, dropout={dp:.1f}, img2txt_mult={img2txt_mult}, txt2img_mult={txt2img_mult} → best Val F1 = {f1:.4f}, best Val Acc = {acc:.4f}, best Val AUROC = {auroc:.4f}\\n\")\n",
        "    results.append((f1, acc, auroc, lr, dp, img2txt_mult, txt2img_mult))  # Store all metrics\n",
        "\n",
        "# 4) pick best\n",
        "best_result = max(results, key=lambda x: x[0])  # Find best by F1 score\n",
        "best_f1, best_acc, best_auroc, best_lr, best_dp, best_img2txt_mult, best_txt2img_mult = best_result\n",
        "\n",
        "# Print Best Configuration with LR Multipliers and AUROC:\n",
        "print(f\">>> Best config by F1: lr={best_lr:.0e}, dropout={best_dp:.1f}, img2txt_mult={best_img2txt_mult}, txt2img_mult={best_txt2img_mult} with Val F1={best_f1:.4f} (Acc={best_acc:.4f}, AUROC={best_auroc:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2zlQkaVj6UR",
        "outputId": "bba7b3f5-2b8c-44b4-f7d0-4e17a4651199"
      },
      "id": "n2zlQkaVj6UR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " → lr=1e-03, dropout=0.1, img2txt_mult=0.5, txt2img_mult=0.5 → best Val F1 = 0.6667, best Val Acc = 0.6933, best Val AUROC = 0.6891\n",
            "\n",
            " → lr=1e-03, dropout=0.1, img2txt_mult=0.5, txt2img_mult=1.0 → best Val F1 = 0.6667, best Val Acc = 0.6800, best Val AUROC = 0.6720\n",
            "\n",
            " → lr=1e-03, dropout=0.1, img2txt_mult=0.5, txt2img_mult=2.0 → best Val F1 = 0.6667, best Val Acc = 0.6800, best Val AUROC = 0.6774\n",
            "\n",
            " → lr=1e-03, dropout=0.1, img2txt_mult=0.5, txt2img_mult=5.0 → best Val F1 = 0.6977, best Val Acc = 0.6800, best Val AUROC = 0.6774\n",
            "\n",
            " → lr=1e-03, dropout=0.1, img2txt_mult=1.0, txt2img_mult=0.5 → best Val F1 = 0.6479, best Val Acc = 0.6667, best Val AUROC = 0.6656\n",
            "\n",
            " → lr=1e-03, dropout=0.1, img2txt_mult=1.0, txt2img_mult=1.0 → best Val F1 = 0.6667, best Val Acc = 0.6667, best Val AUROC = 0.6645\n",
            "\n",
            " → lr=1e-03, dropout=0.1, img2txt_mult=1.0, txt2img_mult=2.0 → best Val F1 = 0.6897, best Val Acc = 0.7067, best Val AUROC = 0.6998\n",
            "\n",
            " → lr=1e-03, dropout=0.1, img2txt_mult=1.0, txt2img_mult=5.0 → best Val F1 = 0.6813, best Val Acc = 0.6800, best Val AUROC = 0.6774\n",
            "\n",
            " → lr=1e-03, dropout=0.1, img2txt_mult=2.0, txt2img_mult=0.5 → best Val F1 = 0.6667, best Val Acc = 0.6667, best Val AUROC = 0.6656\n",
            "\n",
            " → lr=1e-03, dropout=0.1, img2txt_mult=2.0, txt2img_mult=1.0 → best Val F1 = 0.6585, best Val Acc = 0.7067, best Val AUROC = 0.7019\n",
            "\n",
            " → lr=1e-03, dropout=0.1, img2txt_mult=2.0, txt2img_mult=2.0 → best Val F1 = 0.6667, best Val Acc = 0.6800, best Val AUROC = 0.6741\n",
            "\n",
            " → lr=1e-03, dropout=0.1, img2txt_mult=2.0, txt2img_mult=5.0 → best Val F1 = 0.6667, best Val Acc = 0.6933, best Val AUROC = 0.6912\n",
            "\n",
            " → lr=1e-03, dropout=0.1, img2txt_mult=3.0, txt2img_mult=0.5 → best Val F1 = 0.6667, best Val Acc = 0.6933, best Val AUROC = 0.6912\n",
            "\n",
            " → lr=1e-03, dropout=0.1, img2txt_mult=3.0, txt2img_mult=1.0 → best Val F1 = 0.6914, best Val Acc = 0.6800, best Val AUROC = 0.6795\n",
            "\n",
            " → lr=1e-03, dropout=0.1, img2txt_mult=3.0, txt2img_mult=2.0 → best Val F1 = 0.6667, best Val Acc = 0.6800, best Val AUROC = 0.6784\n",
            "\n",
            " → lr=1e-03, dropout=0.1, img2txt_mult=3.0, txt2img_mult=5.0 → best Val F1 = 0.6582, best Val Acc = 0.6800, best Val AUROC = 0.6763\n",
            "\n",
            " → lr=1e-03, dropout=0.2, img2txt_mult=0.5, txt2img_mult=0.5 → best Val F1 = 0.6667, best Val Acc = 0.6800, best Val AUROC = 0.6784\n",
            "\n",
            " → lr=1e-03, dropout=0.2, img2txt_mult=0.5, txt2img_mult=1.0 → best Val F1 = 0.6829, best Val Acc = 0.6533, best Val AUROC = 0.6581\n",
            "\n",
            " → lr=1e-03, dropout=0.2, img2txt_mult=0.5, txt2img_mult=2.0 → best Val F1 = 0.6818, best Val Acc = 0.6667, best Val AUROC = 0.6635\n",
            "\n",
            " → lr=1e-03, dropout=0.2, img2txt_mult=0.5, txt2img_mult=5.0 → best Val F1 = 0.6824, best Val Acc = 0.6933, best Val AUROC = 0.6902\n",
            "\n",
            " → lr=1e-03, dropout=0.2, img2txt_mult=1.0, txt2img_mult=0.5 → best Val F1 = 0.7013, best Val Acc = 0.6933, best Val AUROC = 0.6955\n",
            "\n",
            " → lr=1e-03, dropout=0.2, img2txt_mult=1.0, txt2img_mult=1.0 → best Val F1 = 0.6585, best Val Acc = 0.6933, best Val AUROC = 0.6902\n",
            "\n",
            " → lr=1e-03, dropout=0.2, img2txt_mult=1.0, txt2img_mult=2.0 → best Val F1 = 0.7253, best Val Acc = 0.6667, best Val AUROC = 0.6763\n",
            "\n",
            " → lr=1e-03, dropout=0.2, img2txt_mult=1.0, txt2img_mult=5.0 → best Val F1 = 0.6824, best Val Acc = 0.6667, best Val AUROC = 0.6688\n",
            "\n",
            " → lr=1e-03, dropout=0.2, img2txt_mult=2.0, txt2img_mult=0.5 → best Val F1 = 0.6667, best Val Acc = 0.6800, best Val AUROC = 0.6795\n",
            "\n",
            " → lr=1e-03, dropout=0.2, img2txt_mult=2.0, txt2img_mult=1.0 → best Val F1 = 0.6750, best Val Acc = 0.6667, best Val AUROC = 0.6677\n",
            "\n",
            " → lr=1e-03, dropout=0.2, img2txt_mult=2.0, txt2img_mult=2.0 → best Val F1 = 0.6667, best Val Acc = 0.6800, best Val AUROC = 0.6763\n",
            "\n",
            " → lr=1e-03, dropout=0.2, img2txt_mult=2.0, txt2img_mult=5.0 → best Val F1 = 0.6914, best Val Acc = 0.6667, best Val AUROC = 0.6709\n",
            "\n",
            " → lr=1e-03, dropout=0.2, img2txt_mult=3.0, txt2img_mult=0.5 → best Val F1 = 0.6957, best Val Acc = 0.7067, best Val AUROC = 0.7019\n",
            "\n",
            " → lr=1e-03, dropout=0.2, img2txt_mult=3.0, txt2img_mult=1.0 → best Val F1 = 0.6750, best Val Acc = 0.6667, best Val AUROC = 0.6667\n",
            "\n",
            " → lr=1e-03, dropout=0.2, img2txt_mult=3.0, txt2img_mult=2.0 → best Val F1 = 0.6835, best Val Acc = 0.6667, best Val AUROC = 0.6699\n",
            "\n",
            " → lr=1e-03, dropout=0.2, img2txt_mult=3.0, txt2img_mult=5.0 → best Val F1 = 0.6667, best Val Acc = 0.6800, best Val AUROC = 0.6774\n",
            "\n",
            " → lr=1e-03, dropout=0.3, img2txt_mult=0.5, txt2img_mult=0.5 → best Val F1 = 0.6742, best Val Acc = 0.6533, best Val AUROC = 0.6528\n",
            "\n",
            " → lr=1e-03, dropout=0.3, img2txt_mult=0.5, txt2img_mult=1.0 → best Val F1 = 0.6824, best Val Acc = 0.6667, best Val AUROC = 0.6667\n",
            "\n",
            " → lr=1e-03, dropout=0.3, img2txt_mult=0.5, txt2img_mult=2.0 → best Val F1 = 0.6804, best Val Acc = 0.6667, best Val AUROC = 0.6667\n",
            "\n",
            " → lr=1e-03, dropout=0.3, img2txt_mult=0.5, txt2img_mult=5.0 → best Val F1 = 0.6667, best Val Acc = 0.6667, best Val AUROC = 0.6645\n",
            "\n",
            " → lr=1e-03, dropout=0.3, img2txt_mult=1.0, txt2img_mult=0.5 → best Val F1 = 0.6667, best Val Acc = 0.6667, best Val AUROC = 0.6635\n",
            "\n",
            " → lr=1e-03, dropout=0.3, img2txt_mult=1.0, txt2img_mult=1.0 → best Val F1 = 0.6914, best Val Acc = 0.6800, best Val AUROC = 0.6763\n",
            "\n",
            " → lr=1e-03, dropout=0.3, img2txt_mult=1.0, txt2img_mult=2.0 → best Val F1 = 0.6667, best Val Acc = 0.6667, best Val AUROC = 0.6656\n",
            "\n",
            " → lr=1e-03, dropout=0.3, img2txt_mult=1.0, txt2img_mult=5.0 → best Val F1 = 0.7059, best Val Acc = 0.6933, best Val AUROC = 0.6902\n",
            "\n",
            " → lr=1e-03, dropout=0.3, img2txt_mult=2.0, txt2img_mult=0.5 → best Val F1 = 0.6824, best Val Acc = 0.6667, best Val AUROC = 0.6656\n",
            "\n",
            " → lr=1e-03, dropout=0.3, img2txt_mult=2.0, txt2img_mult=1.0 → best Val F1 = 0.6667, best Val Acc = 0.6667, best Val AUROC = 0.6635\n",
            "\n",
            " → lr=1e-03, dropout=0.3, img2txt_mult=2.0, txt2img_mult=2.0 → best Val F1 = 0.6667, best Val Acc = 0.6667, best Val AUROC = 0.6667\n",
            "\n",
            " → lr=1e-03, dropout=0.3, img2txt_mult=2.0, txt2img_mult=5.0 → best Val F1 = 0.6829, best Val Acc = 0.6667, best Val AUROC = 0.6677\n",
            "\n",
            " → lr=1e-03, dropout=0.3, img2txt_mult=3.0, txt2img_mult=0.5 → best Val F1 = 0.6966, best Val Acc = 0.6933, best Val AUROC = 0.6859\n",
            "\n",
            " → lr=1e-03, dropout=0.3, img2txt_mult=3.0, txt2img_mult=1.0 → best Val F1 = 0.6747, best Val Acc = 0.6667, best Val AUROC = 0.6635\n",
            "\n",
            " → lr=1e-03, dropout=0.3, img2txt_mult=3.0, txt2img_mult=2.0 → best Val F1 = 0.6500, best Val Acc = 0.6800, best Val AUROC = 0.6774\n",
            "\n",
            " → lr=1e-03, dropout=0.3, img2txt_mult=3.0, txt2img_mult=5.0 → best Val F1 = 0.6939, best Val Acc = 0.6667, best Val AUROC = 0.6656\n",
            "\n",
            " → lr=1e-04, dropout=0.1, img2txt_mult=0.5, txt2img_mult=0.5 → best Val F1 = 0.6842, best Val Acc = 0.6800, best Val AUROC = 0.6816\n",
            "\n",
            " → lr=1e-04, dropout=0.1, img2txt_mult=0.5, txt2img_mult=1.0 → best Val F1 = 0.6389, best Val Acc = 0.6667, best Val AUROC = 0.6645\n",
            "\n",
            " → lr=1e-04, dropout=0.1, img2txt_mult=0.5, txt2img_mult=2.0 → best Val F1 = 0.6301, best Val Acc = 0.6533, best Val AUROC = 0.6506\n",
            "\n",
            " → lr=1e-04, dropout=0.1, img2txt_mult=0.5, txt2img_mult=5.0 → best Val F1 = 0.6585, best Val Acc = 0.6533, best Val AUROC = 0.6496\n",
            "\n",
            " → lr=1e-04, dropout=0.1, img2txt_mult=1.0, txt2img_mult=0.5 → best Val F1 = 0.6571, best Val Acc = 0.6800, best Val AUROC = 0.6784\n",
            "\n",
            " → lr=1e-04, dropout=0.1, img2txt_mult=1.0, txt2img_mult=1.0 → best Val F1 = 0.6933, best Val Acc = 0.6933, best Val AUROC = 0.6944\n",
            "\n",
            " → lr=1e-04, dropout=0.1, img2txt_mult=1.0, txt2img_mult=2.0 → best Val F1 = 0.6571, best Val Acc = 0.6800, best Val AUROC = 0.6784\n",
            "\n",
            " → lr=1e-04, dropout=0.1, img2txt_mult=1.0, txt2img_mult=5.0 → best Val F1 = 0.6571, best Val Acc = 0.6800, best Val AUROC = 0.6784\n",
            "\n",
            " → lr=1e-04, dropout=0.1, img2txt_mult=2.0, txt2img_mult=0.5 → best Val F1 = 0.7000, best Val Acc = 0.6800, best Val AUROC = 0.6838\n",
            "\n",
            " → lr=1e-04, dropout=0.1, img2txt_mult=2.0, txt2img_mult=1.0 → best Val F1 = 0.6579, best Val Acc = 0.6667, best Val AUROC = 0.6645\n",
            "\n",
            " → lr=1e-04, dropout=0.1, img2txt_mult=2.0, txt2img_mult=2.0 → best Val F1 = 0.6588, best Val Acc = 0.6667, best Val AUROC = 0.6656\n",
            "\n",
            " → lr=1e-04, dropout=0.1, img2txt_mult=2.0, txt2img_mult=5.0 → best Val F1 = 0.6579, best Val Acc = 0.6667, best Val AUROC = 0.6635\n",
            "\n",
            " → lr=1e-04, dropout=0.1, img2txt_mult=3.0, txt2img_mult=0.5 → best Val F1 = 0.7179, best Val Acc = 0.7067, best Val AUROC = 0.7094\n",
            "\n",
            " → lr=1e-04, dropout=0.1, img2txt_mult=3.0, txt2img_mult=1.0 → best Val F1 = 0.6897, best Val Acc = 0.6800, best Val AUROC = 0.6784\n",
            "\n",
            " → lr=1e-04, dropout=0.1, img2txt_mult=3.0, txt2img_mult=2.0 → best Val F1 = 0.6824, best Val Acc = 0.6933, best Val AUROC = 0.6891\n",
            "\n",
            " → lr=1e-04, dropout=0.1, img2txt_mult=3.0, txt2img_mult=5.0 → best Val F1 = 0.6757, best Val Acc = 0.7067, best Val AUROC = 0.6998\n",
            "\n",
            " → lr=1e-04, dropout=0.2, img2txt_mult=0.5, txt2img_mult=0.5 → best Val F1 = 0.6667, best Val Acc = 0.6800, best Val AUROC = 0.6774\n",
            "\n",
            " → lr=1e-04, dropout=0.2, img2txt_mult=0.5, txt2img_mult=1.0 → best Val F1 = 0.6582, best Val Acc = 0.6667, best Val AUROC = 0.6645\n",
            "\n",
            " → lr=1e-04, dropout=0.2, img2txt_mult=0.5, txt2img_mult=2.0 → best Val F1 = 0.6667, best Val Acc = 0.6800, best Val AUROC = 0.6763\n",
            "\n",
            " → lr=1e-04, dropout=0.2, img2txt_mult=0.5, txt2img_mult=5.0 → best Val F1 = 0.6593, best Val Acc = 0.6533, best Val AUROC = 0.6506\n",
            "\n",
            " → lr=1e-04, dropout=0.2, img2txt_mult=1.0, txt2img_mult=0.5 → best Val F1 = 0.6757, best Val Acc = 0.6800, best Val AUROC = 0.6806\n",
            "\n",
            " → lr=1e-04, dropout=0.2, img2txt_mult=1.0, txt2img_mult=1.0 → best Val F1 = 0.6667, best Val Acc = 0.6667, best Val AUROC = 0.6645\n",
            "\n",
            " → lr=1e-04, dropout=0.2, img2txt_mult=1.0, txt2img_mult=2.0 → best Val F1 = 0.6575, best Val Acc = 0.6667, best Val AUROC = 0.6667\n",
            "\n",
            " → lr=1e-04, dropout=0.2, img2txt_mult=1.0, txt2img_mult=5.0 → best Val F1 = 0.6667, best Val Acc = 0.6800, best Val AUROC = 0.6795\n",
            "\n",
            " → lr=1e-04, dropout=0.2, img2txt_mult=2.0, txt2img_mult=0.5 → best Val F1 = 0.6905, best Val Acc = 0.6800, best Val AUROC = 0.6795\n",
            "\n",
            " → lr=1e-04, dropout=0.2, img2txt_mult=2.0, txt2img_mult=1.0 → best Val F1 = 0.6500, best Val Acc = 0.6933, best Val AUROC = 0.6891\n",
            "\n",
            " → lr=1e-04, dropout=0.2, img2txt_mult=2.0, txt2img_mult=2.0 → best Val F1 = 0.6761, best Val Acc = 0.6933, best Val AUROC = 0.6923\n",
            "\n",
            " → lr=1e-04, dropout=0.2, img2txt_mult=2.0, txt2img_mult=5.0 → best Val F1 = 0.6849, best Val Acc = 0.6933, best Val AUROC = 0.6934\n",
            "\n",
            " → lr=1e-04, dropout=0.2, img2txt_mult=3.0, txt2img_mult=0.5 → best Val F1 = 0.7126, best Val Acc = 0.6933, best Val AUROC = 0.6902\n",
            "\n",
            " → lr=1e-04, dropout=0.2, img2txt_mult=3.0, txt2img_mult=1.0 → best Val F1 = 0.6742, best Val Acc = 0.6667, best Val AUROC = 0.6645\n",
            "\n",
            " → lr=1e-04, dropout=0.2, img2txt_mult=3.0, txt2img_mult=2.0 → best Val F1 = 0.6486, best Val Acc = 0.6800, best Val AUROC = 0.6752\n",
            "\n",
            " → lr=1e-04, dropout=0.2, img2txt_mult=3.0, txt2img_mult=5.0 → best Val F1 = 0.6582, best Val Acc = 0.6667, best Val AUROC = 0.6667\n",
            "\n",
            " → lr=1e-04, dropout=0.3, img2txt_mult=0.5, txt2img_mult=0.5 → best Val F1 = 0.6667, best Val Acc = 0.6800, best Val AUROC = 0.6795\n",
            "\n",
            " → lr=1e-04, dropout=0.3, img2txt_mult=0.5, txt2img_mult=1.0 → best Val F1 = 0.6567, best Val Acc = 0.6933, best Val AUROC = 0.6902\n",
            "\n",
            " → lr=1e-04, dropout=0.3, img2txt_mult=0.5, txt2img_mult=2.0 → best Val F1 = 0.6667, best Val Acc = 0.6800, best Val AUROC = 0.6795\n",
            "\n",
            " → lr=1e-04, dropout=0.3, img2txt_mult=0.5, txt2img_mult=5.0 → best Val F1 = 0.6747, best Val Acc = 0.6800, best Val AUROC = 0.6774\n",
            "\n",
            " → lr=1e-04, dropout=0.3, img2txt_mult=1.0, txt2img_mult=0.5 → best Val F1 = 0.6579, best Val Acc = 0.6800, best Val AUROC = 0.6774\n",
            "\n",
            " → lr=1e-04, dropout=0.3, img2txt_mult=1.0, txt2img_mult=1.0 → best Val F1 = 0.6579, best Val Acc = 0.6800, best Val AUROC = 0.6774\n",
            "\n",
            " → lr=1e-04, dropout=0.3, img2txt_mult=1.0, txt2img_mult=2.0 → best Val F1 = 0.6739, best Val Acc = 0.6933, best Val AUROC = 0.6902\n",
            "\n",
            " → lr=1e-04, dropout=0.3, img2txt_mult=1.0, txt2img_mult=5.0 → best Val F1 = 0.6494, best Val Acc = 0.6667, best Val AUROC = 0.6645\n",
            "\n",
            " → lr=1e-04, dropout=0.3, img2txt_mult=2.0, txt2img_mult=0.5 → best Val F1 = 0.6667, best Val Acc = 0.6933, best Val AUROC = 0.6891\n",
            "\n",
            " → lr=1e-04, dropout=0.3, img2txt_mult=2.0, txt2img_mult=1.0 → best Val F1 = 0.6667, best Val Acc = 0.6800, best Val AUROC = 0.6784\n",
            "\n",
            " → lr=1e-04, dropout=0.3, img2txt_mult=2.0, txt2img_mult=2.0 → best Val F1 = 0.6588, best Val Acc = 0.6800, best Val AUROC = 0.6752\n",
            "\n",
            " → lr=1e-04, dropout=0.3, img2txt_mult=2.0, txt2img_mult=5.0 → best Val F1 = 0.6582, best Val Acc = 0.6667, best Val AUROC = 0.6635\n",
            "\n",
            " → lr=1e-04, dropout=0.3, img2txt_mult=3.0, txt2img_mult=0.5 → best Val F1 = 0.6923, best Val Acc = 0.6933, best Val AUROC = 0.6902\n",
            "\n",
            " → lr=1e-04, dropout=0.3, img2txt_mult=3.0, txt2img_mult=1.0 → best Val F1 = 0.6585, best Val Acc = 0.6800, best Val AUROC = 0.6752\n",
            "\n",
            " → lr=1e-04, dropout=0.3, img2txt_mult=3.0, txt2img_mult=2.0 → best Val F1 = 0.6579, best Val Acc = 0.6800, best Val AUROC = 0.6774\n",
            "\n",
            " → lr=1e-04, dropout=0.3, img2txt_mult=3.0, txt2img_mult=5.0 → best Val F1 = 0.6667, best Val Acc = 0.6800, best Val AUROC = 0.6763\n",
            "\n",
            " → lr=1e-05, dropout=0.1, img2txt_mult=0.5, txt2img_mult=0.5 → best Val F1 = 0.5946, best Val Acc = 0.6000, best Val AUROC = 0.6004\n",
            "\n",
            " → lr=1e-05, dropout=0.1, img2txt_mult=0.5, txt2img_mult=1.0 → best Val F1 = 0.6216, best Val Acc = 0.6267, best Val AUROC = 0.6271\n",
            "\n",
            " → lr=1e-05, dropout=0.1, img2txt_mult=0.5, txt2img_mult=2.0 → best Val F1 = 0.6216, best Val Acc = 0.6267, best Val AUROC = 0.6271\n",
            "\n",
            " → lr=1e-05, dropout=0.1, img2txt_mult=0.5, txt2img_mult=5.0 → best Val F1 = 0.6567, best Val Acc = 0.6933, best Val AUROC = 0.6902\n",
            "\n",
            " → lr=1e-05, dropout=0.1, img2txt_mult=1.0, txt2img_mult=0.5 → best Val F1 = 0.5897, best Val Acc = 0.5867, best Val AUROC = 0.5865\n",
            "\n",
            " → lr=1e-05, dropout=0.1, img2txt_mult=1.0, txt2img_mult=1.0 → best Val F1 = 0.6301, best Val Acc = 0.6400, best Val AUROC = 0.6400\n",
            "\n",
            " → lr=1e-05, dropout=0.1, img2txt_mult=1.0, txt2img_mult=2.0 → best Val F1 = 0.6234, best Val Acc = 0.6400, best Val AUROC = 0.6378\n",
            "\n",
            " → lr=1e-05, dropout=0.1, img2txt_mult=1.0, txt2img_mult=5.0 → best Val F1 = 0.6471, best Val Acc = 0.6800, best Val AUROC = 0.6774\n",
            "\n",
            " → lr=1e-05, dropout=0.1, img2txt_mult=2.0, txt2img_mult=0.5 → best Val F1 = 0.6053, best Val Acc = 0.6000, best Val AUROC = 0.6015\n",
            "\n",
            " → lr=1e-05, dropout=0.1, img2txt_mult=2.0, txt2img_mult=1.0 → best Val F1 = 0.6486, best Val Acc = 0.6533, best Val AUROC = 0.6538\n",
            "\n",
            " → lr=1e-05, dropout=0.1, img2txt_mult=2.0, txt2img_mult=2.0 → best Val F1 = 0.6575, best Val Acc = 0.6667, best Val AUROC = 0.6667\n",
            "\n",
            " → lr=1e-05, dropout=0.1, img2txt_mult=2.0, txt2img_mult=5.0 → best Val F1 = 0.6479, best Val Acc = 0.6667, best Val AUROC = 0.6656\n",
            "\n",
            " → lr=1e-05, dropout=0.1, img2txt_mult=3.0, txt2img_mult=0.5 → best Val F1 = 0.6420, best Val Acc = 0.6267, best Val AUROC = 0.6239\n",
            "\n",
            " → lr=1e-05, dropout=0.1, img2txt_mult=3.0, txt2img_mult=1.0 → best Val F1 = 0.6486, best Val Acc = 0.6533, best Val AUROC = 0.6538\n",
            "\n",
            " → lr=1e-05, dropout=0.1, img2txt_mult=3.0, txt2img_mult=2.0 → best Val F1 = 0.6667, best Val Acc = 0.6667, best Val AUROC = 0.6677\n",
            "\n",
            " → lr=1e-05, dropout=0.1, img2txt_mult=3.0, txt2img_mult=5.0 → best Val F1 = 0.6389, best Val Acc = 0.6667, best Val AUROC = 0.6645\n",
            "\n",
            " → lr=1e-05, dropout=0.2, img2txt_mult=0.5, txt2img_mult=0.5 → best Val F1 = 0.6095, best Val Acc = 0.5733, best Val AUROC = 0.5726\n",
            "\n",
            " → lr=1e-05, dropout=0.2, img2txt_mult=0.5, txt2img_mult=1.0 → best Val F1 = 0.6301, best Val Acc = 0.6400, best Val AUROC = 0.6400\n",
            "\n",
            " → lr=1e-05, dropout=0.2, img2txt_mult=0.5, txt2img_mult=2.0 → best Val F1 = 0.5753, best Val Acc = 0.5867, best Val AUROC = 0.5865\n",
            "\n",
            " → lr=1e-05, dropout=0.2, img2txt_mult=0.5, txt2img_mult=5.0 → best Val F1 = 0.6269, best Val Acc = 0.6667, best Val AUROC = 0.6635\n",
            "\n",
            " → lr=1e-05, dropout=0.2, img2txt_mult=1.0, txt2img_mult=0.5 → best Val F1 = 0.5753, best Val Acc = 0.5867, best Val AUROC = 0.5865\n",
            "\n",
            " → lr=1e-05, dropout=0.2, img2txt_mult=1.0, txt2img_mult=1.0 → best Val F1 = 0.6053, best Val Acc = 0.6000, best Val AUROC = 0.6015\n",
            "\n",
            " → lr=1e-05, dropout=0.2, img2txt_mult=1.0, txt2img_mult=2.0 → best Val F1 = 0.6111, best Val Acc = 0.6267, best Val AUROC = 0.6261\n",
            "\n",
            " → lr=1e-05, dropout=0.2, img2txt_mult=1.0, txt2img_mult=5.0 → best Val F1 = 0.6389, best Val Acc = 0.6533, best Val AUROC = 0.6528\n",
            "\n",
            " → lr=1e-05, dropout=0.2, img2txt_mult=2.0, txt2img_mult=0.5 → best Val F1 = 0.5753, best Val Acc = 0.5867, best Val AUROC = 0.5865\n",
            "\n",
            " → lr=1e-05, dropout=0.2, img2txt_mult=2.0, txt2img_mult=1.0 → best Val F1 = 0.6197, best Val Acc = 0.6667, best Val AUROC = 0.6581\n",
            "\n",
            " → lr=1e-05, dropout=0.2, img2txt_mult=2.0, txt2img_mult=2.0 → best Val F1 = 0.6579, best Val Acc = 0.6533, best Val AUROC = 0.6549\n",
            "\n",
            " → lr=1e-05, dropout=0.2, img2txt_mult=2.0, txt2img_mult=5.0 → best Val F1 = 0.6667, best Val Acc = 0.6933, best Val AUROC = 0.6912\n",
            "\n",
            " → lr=1e-05, dropout=0.2, img2txt_mult=3.0, txt2img_mult=0.5 → best Val F1 = 0.5974, best Val Acc = 0.6133, best Val AUROC = 0.6100\n",
            "\n",
            " → lr=1e-05, dropout=0.2, img2txt_mult=3.0, txt2img_mult=1.0 → best Val F1 = 0.6389, best Val Acc = 0.6533, best Val AUROC = 0.6528\n",
            "\n",
            " → lr=1e-05, dropout=0.2, img2txt_mult=3.0, txt2img_mult=2.0 → best Val F1 = 0.6579, best Val Acc = 0.6533, best Val AUROC = 0.6549\n",
            "\n",
            " → lr=1e-05, dropout=0.2, img2txt_mult=3.0, txt2img_mult=5.0 → best Val F1 = 0.6471, best Val Acc = 0.6800, best Val AUROC = 0.6774\n",
            "\n",
            " → lr=1e-05, dropout=0.3, img2txt_mult=0.5, txt2img_mult=0.5 → best Val F1 = 0.5634, best Val Acc = 0.5867, best Val AUROC = 0.5855\n",
            "\n",
            " → lr=1e-05, dropout=0.3, img2txt_mult=0.5, txt2img_mult=1.0 → best Val F1 = 0.5946, best Val Acc = 0.6000, best Val AUROC = 0.6004\n",
            "\n",
            " → lr=1e-05, dropout=0.3, img2txt_mult=0.5, txt2img_mult=2.0 → best Val F1 = 0.6087, best Val Acc = 0.6400, best Val AUROC = 0.6378\n",
            "\n",
            " → lr=1e-05, dropout=0.3, img2txt_mult=0.5, txt2img_mult=5.0 → best Val F1 = 0.6286, best Val Acc = 0.6533, best Val AUROC = 0.6517\n",
            "\n",
            " → lr=1e-05, dropout=0.3, img2txt_mult=1.0, txt2img_mult=0.5 → best Val F1 = 0.6061, best Val Acc = 0.6533, best Val AUROC = 0.6496\n",
            "\n",
            " → lr=1e-05, dropout=0.3, img2txt_mult=1.0, txt2img_mult=1.0 → best Val F1 = 0.5974, best Val Acc = 0.5867, best Val AUROC = 0.5887\n",
            "\n",
            " → lr=1e-05, dropout=0.3, img2txt_mult=1.0, txt2img_mult=2.0 → best Val F1 = 0.6027, best Val Acc = 0.6267, best Val AUROC = 0.6250\n",
            "\n",
            " → lr=1e-05, dropout=0.3, img2txt_mult=1.0, txt2img_mult=5.0 → best Val F1 = 0.6197, best Val Acc = 0.6400, best Val AUROC = 0.6389\n",
            "\n",
            " → lr=1e-05, dropout=0.3, img2txt_mult=2.0, txt2img_mult=0.5 → best Val F1 = 0.5600, best Val Acc = 0.5733, best Val AUROC = 0.5726\n",
            "\n",
            " → lr=1e-05, dropout=0.3, img2txt_mult=2.0, txt2img_mult=1.0 → best Val F1 = 0.6111, best Val Acc = 0.6267, best Val AUROC = 0.6261\n",
            "\n",
            " → lr=1e-05, dropout=0.3, img2txt_mult=2.0, txt2img_mult=2.0 → best Val F1 = 0.6301, best Val Acc = 0.6533, best Val AUROC = 0.6517\n",
            "\n",
            " → lr=1e-05, dropout=0.3, img2txt_mult=2.0, txt2img_mult=5.0 → best Val F1 = 0.6479, best Val Acc = 0.6667, best Val AUROC = 0.6656\n",
            "\n",
            " → lr=1e-05, dropout=0.3, img2txt_mult=3.0, txt2img_mult=0.5 → best Val F1 = 0.5789, best Val Acc = 0.6133, best Val AUROC = 0.6090\n",
            "\n",
            " → lr=1e-05, dropout=0.3, img2txt_mult=3.0, txt2img_mult=1.0 → best Val F1 = 0.6197, best Val Acc = 0.6400, best Val AUROC = 0.6389\n",
            "\n",
            " → lr=1e-05, dropout=0.3, img2txt_mult=3.0, txt2img_mult=2.0 → best Val F1 = 0.6486, best Val Acc = 0.6533, best Val AUROC = 0.6538\n",
            "\n",
            " → lr=1e-05, dropout=0.3, img2txt_mult=3.0, txt2img_mult=5.0 → best Val F1 = 0.6400, best Val Acc = 0.6533, best Val AUROC = 0.6528\n",
            "\n",
            ">>> Best config by F1: lr=1e-03, dropout=0.2, img2txt_mult=1.0, txt2img_mult=2.0 with Val F1=0.7253 (Acc=0.6667, AUROC=0.6763)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d84fc459",
      "metadata": {
        "id": "d84fc459"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}