{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 # VQA-RAD Multimodal Pipeline README\
\
## Overview\
This project is a multimodal Visual Question Answering (VQA) pipeline built on the VQA-RAD dataset. It uses pretrained DenseNet121 or ViT for image encoding and BERT or BiLSTM for question encoding, with a co-attention mechanism to fuse modalities. The goal is to predict short medical answers (from a fixed vocabulary) to radiology-related questions based on image-text pairs.\
\
\
## System Requirements\
Python 3.8+\
PyTorch (with CUDA support recommended)\
Transformers (Hugging Face) library\
GPU with at least 16GB VRAM (24GB+ preferred)\
CUDA-capable GPU with compute capability 7.0+ (e.g., NVIDIA V100, A100)\
\
\
## Model Components\
\
- Image Encoder: ViT or DenseNet121\
- Text Encoder: BERT-base  or BiLSTM\
\
\
## Setup Instructions\
\
1. Install required packages : \
	pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\
	pip install transformers open_clip_torch pandas datasets scikit-learn tqdm\
	pip install accelerate evaluate rouge score\
\
2. Download VQA_RAD image Folder.zip and VQA_RAD Datatset Public.json\
3. Run each file that ends with ClosedModel\
\
\
## Evaluation Metrics\
\
- **BLEU, ROUGE-1, ROUGE-2, ROUGE-L, Exact Matching\
\
\
}