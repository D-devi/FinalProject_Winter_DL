{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Imports\n",
        "\n",
        "from datasets import load_dataset\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "import base64\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "from PIL import Image\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import datetime\n",
        "import hashlib"
      ],
      "metadata": {
        "id": "JTwtNau1KO5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Set up Gemini API\n",
        "print(\"Setting up API keys...\")\n",
        "api_keys = [\n",
        "    \"xyz1\",\n",
        "    \"xyz2\",\n",
        "    \"xyz3\",\n",
        "    \"xyz4\",\n",
        "    # Add your additional API keys here\n",
        "]\n",
        "current_key_index = 0\n",
        "print(f\"Loaded {len(api_keys)} API key(s)\")\n",
        "genai.configure(api_key=api_keys[current_key_index])\n",
        "print(\"Initial API key configured successfully\")\n",
        "\n",
        "# Rate limiting variables (tracking per key)\n",
        "API_CALLS = {key: [] for key in api_keys}  # Tracks timestamp of each API call per key\n",
        "KEY_ERRORS = {key: 0 for key in api_keys}  # Tracks errors per key\n",
        "RATE_LIMIT = 60  # Free tier limit: 60 requests per minute\n",
        "MIN_DELAY = 1.2  # Minimum delay between requests\n",
        "\n",
        "# Function to rotate to the next available API key\n",
        "def rotate_api_key():\n",
        "    \"\"\"Switch to the next available API key\"\"\"\n",
        "    global current_key_index\n",
        "\n",
        "    # Move to next key\n",
        "    previous_key = api_keys[current_key_index]\n",
        "    current_key_index = (current_key_index + 1) % len(api_keys)\n",
        "    new_key = api_keys[current_key_index]\n",
        "\n",
        "    # Configure the API with the new key\n",
        "    genai.configure(api_key=new_key)\n",
        "\n",
        "    print(f\"\\nRotated from API key ending in '...{previous_key[-4:]}' to '...{new_key[-4:]}'\")\n",
        "    return new_key\n",
        "\n",
        "# Function to wait if needed to respect rate limits\n",
        "def respect_rate_limit():\n",
        "    \"\"\"Ensure we don't exceed the rate limit by tracking API calls and waiting if needed.\"\"\"\n",
        "    global API_CALLS, current_key_index\n",
        "\n",
        "    current_key = api_keys[current_key_index]\n",
        "    now = time.time()\n",
        "\n",
        "    # Remove API calls older than 1 minute from our tracking list for current key\n",
        "    API_CALLS[current_key] = [t for t in API_CALLS[current_key] if now - t < 60]\n",
        "\n",
        "    # If we're at or near the limit for current key, try rotating keys first\n",
        "    if len(API_CALLS[current_key]) >= RATE_LIMIT - 5:  # Leave a small buffer\n",
        "        # Check if we have other keys with capacity\n",
        "        for idx, key in enumerate(api_keys):\n",
        "            if idx != current_key_index and len([t for t in API_CALLS[key] if now - t < 60]) < RATE_LIMIT - 10:\n",
        "                # Found a key with available capacity\n",
        "                current_key_index = idx\n",
        "                genai.configure(api_key=key)\n",
        "                print(f\"\\nSwitched to API key ending in '...{key[-4:]}' with available capacity\")\n",
        "                return\n",
        "\n",
        "        # If we reach here, all keys are near capacity, so use the current key but wait\n",
        "        wait_time = 60 - (now - API_CALLS[current_key][0]) + random.uniform(1, 3)\n",
        "\n",
        "        # Log the cooldown period\n",
        "        cooldown_end = datetime.datetime.now() + datetime.timedelta(seconds=wait_time)\n",
        "        print(f\"\\nAll keys near rate limit. Cooling down for {wait_time:.1f} seconds until {cooldown_end.strftime('%H:%M:%S')}\")\n",
        "\n",
        "        time.sleep(wait_time)\n",
        "\n",
        "        # After waiting, clear expired entries again\n",
        "        now = time.time()\n",
        "        API_CALLS[current_key] = [t for t in API_CALLS[current_key] if now - t < 60]\n",
        "\n",
        "    # Add a minimum delay between requests regardless\n",
        "    time.sleep(MIN_DELAY)\n",
        "\n",
        "    # Track this API call with the current key\n",
        "    API_CALLS[current_key].append(time.time())\n"
      ],
      "metadata": {
        "id": "A8tZeLsnKS6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Load data\n",
        "\n",
        "split = \"train\"\n",
        "json_file = f\"md5_{split}/vqa_rad_gemini_batch1_{split}.json\"\n",
        "temp_json_file = f\"md5_{split}/vqa_rad_gemini_batch1_{split}_temp.json\"\n",
        "\n",
        "# Load the dataset\n",
        "ds = load_dataset(\"flaviagiammarino/vqa-rad\")\n",
        "\n",
        "def get_image_hash(image):\n",
        "    \"\"\"Generate MD5 hash of an image.\"\"\"\n",
        "    # Convert image to bytes and compute MD5 hash\n",
        "    md5_hash = hashlib.md5(image.tobytes()).hexdigest()\n",
        "    return md5_hash\n",
        "\n",
        "# Create a structured dataset\n",
        "structured_dataset = defaultdict(lambda: {'image': None, 'qa_pairs': []})\n",
        "\n",
        "# Process data set\n",
        "print(f\"Processing {split} set...\")\n",
        "for idx in range(len(ds[split])):\n",
        "    image = ds[split][idx]['image']\n",
        "    img_hash = get_image_hash(image)\n",
        "\n",
        "    if structured_dataset[img_hash]['image'] is None:\n",
        "        structured_dataset[img_hash]['image'] = {\n",
        "            'data': image,\n",
        "            'size': image.size,\n",
        "            'split': split\n",
        "        }\n",
        "\n",
        "    structured_dataset[img_hash]['qa_pairs'].append({\n",
        "        'question': ds[split][idx]['question'],\n",
        "        'answer': ds[split][idx]['answer']\n",
        "    })\n",
        "\n",
        "print(f\"\\nTotal unique images: {len(structured_dataset)}\")\n",
        "total_qa_pairs = sum(len(data['qa_pairs']) for data in structured_dataset.values())\n",
        "print(f\"Total QA pairs: {total_qa_pairs}\")\n",
        "\n",
        "# Create new md5_images directory if it doesn't exist\n",
        "md5_images_dir = \"md5_images\"\n",
        "os.makedirs(md5_images_dir, exist_ok=True)\n",
        "print(f\"Created {md5_images_dir} directory for storing MD5-hashed images\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8rHwlhdVKVP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Generate descriptive answer using Gemini with exponential backoff for retries\n",
        "\n",
        "def generate_descriptive_answer(image_path, question, reference_answer, max_retries=5):\n",
        "    \"\"\"Generate descriptive answer with retry logic for rate limits and transient errors.\"\"\"\n",
        "    global current_key_index\n",
        "\n",
        "    retry_count = 0\n",
        "    base_wait = 2  # Base wait time in seconds for exponential backoff\n",
        "\n",
        "    while retry_count <= max_retries:\n",
        "        try:\n",
        "            # Respect the rate limit before making a call\n",
        "            respect_rate_limit()\n",
        "\n",
        "            # Load the image\n",
        "            image = Image.open(image_path)\n",
        "\n",
        "            # Initialize Gemini 2.0 Flash model\n",
        "            model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "            # Prepare the prompt\n",
        "            prompt = f\"\"\"You are a medical imaging expert. Please analyze this medical image and answer the following question.\n",
        "\n",
        "            Question: {question}\n",
        "            Reference Answer: {reference_answer}\n",
        "\n",
        "            Your response should have the following pattern: <Reference Answer>. Explanation: <a clear explanation supporting the reference answer.>\n",
        "            \"\"\"\n",
        "\n",
        "            # Generate response with max token limit\n",
        "            generation_config = genai.GenerationConfig(\n",
        "                max_output_tokens=100,  # Limit to 100 tokens (approximately 75-100 words)\n",
        "                temperature=0.2,  # Lower temperature for more focused responses\n",
        "            )\n",
        "\n",
        "            response = model.generate_content(\n",
        "                [prompt, image],\n",
        "                generation_config=generation_config\n",
        "            )\n",
        "\n",
        "            return response.text\n",
        "\n",
        "        except Exception as e:\n",
        "            retry_count += 1\n",
        "            error_str = str(e)\n",
        "            current_key = api_keys[current_key_index]\n",
        "\n",
        "            # Track error for current key\n",
        "            KEY_ERRORS[current_key] += 1\n",
        "\n",
        "            # Check if it's a rate limit or quota error\n",
        "            if \"rate limit\" in error_str.lower() or \"quota\" in error_str.lower() or \"429\" in error_str:\n",
        "                print(f\"\\nRate limit or quota error with key ending in '...{current_key[-4:]}'\")\n",
        "\n",
        "                # If we have multiple keys, try rotating to a different key first\n",
        "                if len(api_keys) > 1:\n",
        "                    rotate_api_key()\n",
        "                    # Reset retry count to give the new key a fresh chance\n",
        "                    retry_count = 0\n",
        "                    continue\n",
        "                else:\n",
        "                    # Use exponential backoff with jitter for single key\n",
        "                    wait_time = (base_wait ** retry_count) + random.uniform(1, 5)\n",
        "                    cooldown_end = datetime.datetime.now() + datetime.timedelta(seconds=wait_time)\n",
        "                    print(f\"\\nNo additional keys available. Retry {retry_count}/{max_retries} after {wait_time:.1f}s ({cooldown_end.strftime('%H:%M:%S')})\")\n",
        "                    time.sleep(wait_time)\n",
        "            else:\n",
        "                # For non-rate-limit errors, still use backoff but with less wait time\n",
        "                wait_time = retry_count * 2 + random.uniform(0, 1)\n",
        "                print(f\"\\nError: {error_str}. Retrying in {wait_time:.1f}s ({retry_count}/{max_retries})\")\n",
        "                time.sleep(wait_time)\n",
        "\n",
        "                # If this key has had multiple errors, try rotating\n",
        "                if KEY_ERRORS[current_key] >= 3 and len(api_keys) > 1:\n",
        "                    rotate_api_key()\n",
        "                    # Reset error count for the new key\n",
        "                    retry_count = 0\n",
        "                    continue\n",
        "\n",
        "            # If we've exhausted retries\n",
        "            if retry_count > max_retries:\n",
        "                return f\"Error after {max_retries} retries: {error_str}\"\n",
        "\n",
        "# Initialize empty enhanced dataset\n",
        "enhanced_dataset = {}\n",
        "\n",
        "# Get all image hashes\n",
        "img_hash_list = list(structured_dataset.keys())\n",
        "\n",
        "# Process first 150 images\n",
        "BATCH_SIZE = len(img_hash_list)\n",
        "print(f\"\\nProcessing first {BATCH_SIZE} images...\")\n",
        "\n",
        "\n",
        "# Process images\n",
        "processed_count = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for img_idx, img_hash in enumerate(tqdm(img_hash_list[:BATCH_SIZE], desc=\"Processing Images\")):\n",
        "    img_data = structured_dataset[img_hash]\n",
        "\n",
        "    # Save the image to disk (use MD5 hash in filename)\n",
        "    image_filename = f\"{md5_images_dir}/image_md5_{img_hash}.png\"\n",
        "    img_data['image']['data'].save(image_filename)\n",
        "\n",
        "    # Process this image's Q&A pairs\n",
        "    enhanced_image_data = {\n",
        "        'image_info': {\n",
        "            'size': img_data['image']['size'],\n",
        "            'split': img_data['image']['split']\n",
        "        },\n",
        "        'image_path': image_filename,  # Store the path with MD5 hash\n",
        "        'qa_pairs': []\n",
        "    }\n",
        "\n",
        "    qa_count = len(img_data['qa_pairs'])\n",
        "    print(f\"\\nProcessing image {img_idx + 1}/{BATCH_SIZE} (Total: {img_idx + 1}/{len(structured_dataset)})\")\n",
        "    print(f\"QA pairs in this image: {qa_count}\")\n",
        "\n",
        "    # Calculate and show ETA for this image\n",
        "    eta_seconds = qa_count * (MIN_DELAY + 0.5)  # Rough estimate\n",
        "    eta_time = datetime.datetime.now() + datetime.timedelta(seconds=eta_seconds)\n",
        "    print(f\"Estimated completion time for this image: {eta_time.strftime('%H:%M:%S')}\")\n",
        "\n",
        "    for i, qa in enumerate(tqdm(img_data['qa_pairs'], desc=f\"Processing QA pairs\")):\n",
        "        try:\n",
        "            # Display current progress with timestamps\n",
        "            now = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
        "            print(f\"[{now}] Processing QA pair {i+1}/{qa_count}: {qa['question'][:50]}...\")\n",
        "\n",
        "            # Generate descriptive answer with retry logic\n",
        "            descriptive_answer = generate_descriptive_answer(image_filename, qa['question'], qa['answer'])\n",
        "\n",
        "            # Store both original and generated answers\n",
        "            enhanced_qa = {\n",
        "                'question': qa['question'],\n",
        "                'original_answer': qa['answer'],\n",
        "                'descriptive_answer': descriptive_answer,\n",
        "                'split': img_data['image']['split']  # Include train/test split label\n",
        "            }\n",
        "\n",
        "            enhanced_image_data['qa_pairs'].append(enhanced_qa)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nUnhandled error processing QA pair {i+1} for image {img_idx + 1}: {str(e)}\")\n",
        "            # Add partial data\n",
        "            enhanced_qa = {\n",
        "                'question': qa['question'],\n",
        "                'original_answer': qa['answer'],\n",
        "                'descriptive_answer': f\"Error generating answer: {str(e)}\",\n",
        "                'split': img_data['image']['split']  # Include train/test split label\n",
        "            }\n",
        "            enhanced_image_data['qa_pairs'].append(enhanced_qa)\n",
        "\n",
        "        # Save after every 5 QA pairs to avoid data loss\n",
        "        if (i + 1) % 5 == 0:\n",
        "            # Make a temporary copy of the dataset with the current progress\n",
        "            temp_dataset = enhanced_dataset.copy()\n",
        "            temp_dataset[img_hash] = enhanced_image_data\n",
        "            with open(temp_json_file, 'w') as f:\n",
        "                json.dump(temp_dataset, f)\n",
        "\n",
        "    # Insert a cooldown period after each image to be extra safe with rate limits\n",
        "    cooldown = random.uniform(5, 10)\n",
        "    print(f\"\\nCooldown period of {cooldown:.1f}s after completing image {img_idx + 1}\")\n",
        "    time.sleep(cooldown)\n",
        "\n",
        "    # Save this image's data to the enhanced dataset\n",
        "    enhanced_dataset[img_hash] = enhanced_image_data\n",
        "    processed_count += 1\n",
        "\n",
        "    # Save intermediate results every 5 images or at the end\n",
        "    if processed_count % 5 == 0 or processed_count == BATCH_SIZE:\n",
        "        with open(json_file, 'w') as f:\n",
        "            json.dump(enhanced_dataset, f, indent=2)\n",
        "\n",
        "        # Calculate and display statistics about progress\n",
        "        elapsed = time.time() - start_time\n",
        "        images_left = BATCH_SIZE - processed_count\n",
        "        avg_time_per_image = elapsed / processed_count if processed_count > 0 else 0\n",
        "        eta_seconds = images_left * avg_time_per_image\n",
        "        eta_time = datetime.datetime.now() + datetime.timedelta(seconds=eta_seconds)\n",
        "\n",
        "        print(f\"\\nProgress: {processed_count}/{BATCH_SIZE} images processed ({processed_count/BATCH_SIZE*100:.1f}%)\")\n",
        "        print(f\"Time elapsed: {elapsed/60:.1f} minutes\")\n",
        "        print(f\"Estimated time remaining: {eta_seconds/60:.1f} minutes (completion around {eta_time.strftime('%H:%M:%S')})\")\n",
        "        print(f\"Results saved after processing {processed_count}/{BATCH_SIZE} images\")\n"
      ],
      "metadata": {
        "id": "YuWogaBtKyFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjfDEImUJhPK"
      },
      "outputs": [],
      "source": [
        "# @title 5. Save the final enhanced dataset\n",
        "\n",
        "with open(json_file, 'w') as f:\n",
        "    json.dump(enhanced_dataset, f, indent=2)\n",
        "\n",
        "# Calculate and show final statistics\n",
        "total_qa_processed = sum(len(data['qa_pairs']) for data in enhanced_dataset.values())\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nComplete enhanced dataset has been saved to '{json_file}'\")\n",
        "print(f\"Processed {processed_count} images with {total_qa_processed} QA pairs\")\n",
        "print(f\"Total processing time: {total_time/60:.1f} minutes ({total_time/3600:.2f} hours)\")\n",
        "print(f\"Average time per QA pair: {total_time/total_qa_processed:.2f} seconds\")\n",
        "\n",
        "# Print sample of the enhanced dataset (just the first image)\n",
        "print(\"\\nSample of Enhanced Dataset (first image):\")\n",
        "first_key = next(iter(enhanced_dataset))\n",
        "print(json.dumps({first_key: enhanced_dataset[first_key]}, indent=2))"
      ]
    }
  ]
}